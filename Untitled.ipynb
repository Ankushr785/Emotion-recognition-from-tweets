{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4230 documents\n",
      "13 classes ['relief', 'anger', 'surprise', 'fun', 'hate', 'boredom', 'empty', 'neutral', 'love', 'worry', 'sadness', 'enthusiasm', 'happiness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/guest-Qi27FR/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:116: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "#important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "#importing stopwords is optional, in this case it decreased accuracy\n",
    "#from nltk.corpus import stopwords\n",
    "import itertools\n",
    "\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "os.chdir('/tmp/guest-Qi27FR')\n",
    "\n",
    "\n",
    "data = pd.read_csv('text_emotion.csv')\n",
    "data = data.iloc[:5000,:]\n",
    "\n",
    "\n",
    "#stopset = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "#comprehensive cleaning\n",
    "for i in range(len(data)):\n",
    "    data.content[i] = re.sub(r\"http\\S+\", \"\", data.content[i])\n",
    "    if len(data.content[i]) == 0:\n",
    "        data.drop(i, inplace = True)\n",
    "    else:\n",
    "        data.content[i] = data.content[i].split()\n",
    "        index = 0\n",
    "        for j in range(len(data.content[i])):\n",
    "            if data.content[i][j][0] == '@':\n",
    "                index = j\n",
    "        data.content[i] = np.delete(data.content[i], index)\n",
    "        if len(data.content[i]) == 0:\n",
    "            data.drop(i, inplace = True)\n",
    "        else:\n",
    "            words = data.content[i][0]\n",
    "            for k in range(len(data.content[i])-1):\n",
    "                words+= \" \" + data.content[i][k+1]\n",
    "            data.content[i] = words\n",
    "            data.content[i] = re.sub(r'[^\\w]', ' ', data.content[i])\n",
    "            if len(data.content[i]) == 0:\n",
    "                data.drop(i, inplace = True)\n",
    "            else:\n",
    "                data.content[i] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(data.content[i]))\n",
    "                data.content[i] = data.content[i].replace(\"'\", \"\")\n",
    "                data.content[i] = nltk.tokenize.word_tokenize(data.content[i])\n",
    "                #data.content[i] = [w for w in data.content[i] if not w in stopset]\n",
    "                for j in range(len(data.content[i])):\n",
    "                    data.content[i][j] = lem.lemmatize(data.content[i][j], \"v\")\n",
    "                if len(data.content[i]) == 0:\n",
    "                    data.drop(i, inplace = True)\n",
    "\n",
    "        \n",
    "data = data.reset_index(drop=True)\n",
    "for i in range(len(data)):\n",
    "    words = data.content[i][0]\n",
    "    for j in range(len(data.content[i])-1):\n",
    "        words+= ' ' + data.content[i][j+1]\n",
    "    data.content[i] = words\n",
    "    \n",
    "le_train = data.iloc[:int(np.round(len(data)*.85)), :]\n",
    "val = data.iloc[int(np.round(len(data)*.85)):,:].reset_index(drop = True)\n",
    "    \n",
    "training_data = []\n",
    "for i in range(len(le_train)):\n",
    "    training_data.append({\"class\": le_train.sentiment[i], \"sentence\": le_train.content[i]})\n",
    "    \n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "# loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    w = nltk.word_tokenize(pattern['sentence'])\n",
    "    words.extend(w)\n",
    "    documents.append((w, pattern['class']))\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "\n",
    "classes = list(set(classes))\n",
    "\n",
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    " \n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "    return(np.array(bag))\n",
    "\n",
    "def model(sentence):\n",
    "    x = bow(sentence.lower(), words)\n",
    "    # input layer is our bag of words\n",
    "    l0 = x\n",
    "    # matrix multiplication of input and hidden layer\n",
    "    l1 = sigmoid(np.dot(l0, synapse_0))\n",
    "    # output layer\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2\n",
    "\n",
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=5000, dropout=False, dropout_percent=0.2):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 1000) == 0 and j > 500:\n",
    "            # if this 1k iteration's error is greater than the last iteration, break out\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    # persist synapses\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "\n",
    "    \n",
    "X = np.array(training)\n",
    "y = np.array(output)\n",
    "\n",
    "train(X, y, hidden_neurons=20, alpha=0.1, epochs=1000, dropout=False, dropout_percent=0.2)\n",
    "\n",
    "# load our calculated synapse values\n",
    "synapse_file = 'synapses.json' \n",
    "with open(synapse_file) as data_file: \n",
    "    synapse = json.load(data_file) \n",
    "    synapse_0 = np.asarray(synapse['synapse0']) \n",
    "    synapse_1 = np.asarray(synapse['synapse1'])\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = model(sentence)\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == np.max(results):\n",
    "            emotion = classes[i]\n",
    "    return emotion\n",
    "    \n",
    "\n",
    "#validation\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(val)):\n",
    "    predictions.append(classify(val.content[i]))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(val.sentiment, predictions))\n",
    "\n",
    "prediction_df = pd.DataFrame({'content':val.content, 'sentiment_predicted':predictions, 'sentimentActual':val.sentiment})\n",
    "\n",
    "prediction_df.to_csv('emotion_recognizer.csv', index = False)\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
